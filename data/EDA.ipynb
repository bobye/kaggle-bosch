{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data proprocess for fast IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load preprocessed train_categorical.csv into np.float (NaN not supported on np.int)\n",
    "start_time = time.time()\n",
    "train_categorical=pd.read_csv('train_categorical_int.csv', dtype=np.float)\n",
    "print('Load train_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 4.6 minutes\n",
    "\n",
    "# Process train_categorical into np.int32 to save memory\n",
    "# \n",
    "train_categorical.fillna(-999, inplace=True)\n",
    "train_categorical=train_categorical.astype(np.int32)\n",
    "print('Process train_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 6.25 minutes\n",
    "\n",
    "# Resave\n",
    "train_categorical.to_hdf('train_categorical_int.h5','table')\n",
    "print('Resave train_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 8.19 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load preprocessed train_categorical.csv into np.float (NaN not supported on np.int)\n",
    "start_time = time.time()\n",
    "test_categorical=pd.read_csv('test_categorical_int.csv', dtype=np.float)\n",
    "print('Load test_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 4.6 minutes\n",
    "\n",
    "# Process train_categorical into np.int32 to save memory\n",
    "# \n",
    "test_categorical.fillna(-999, inplace=True)\n",
    "test_categorical=test_categorical.astype(np.int32)\n",
    "print('Process test_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 6.25 minutes\n",
    "\n",
    "# Resave\n",
    "test_categorical.to_hdf('test_categorical_int.h5','table')\n",
    "print('Resave test_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 8.19 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load numerical features and Response!\n",
    "start_time = time.time()\n",
    "train_numeric=pd.read_csv('train_numeric.csv', dtype=np.float)\n",
    "print('Load train_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 1.8 minutes\n",
    "\n",
    "train_numeric['Id']=train_numeric['Id'].astype(np.int32)\n",
    "train_numeric['Response']=train_numeric['Response'].astype(np.int32)\n",
    "print('Process train_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "train_numeric.to_hdf('train_numeric.h5','table', complevel=1)\n",
    "print('Resave train_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load numerical features and Response!\n",
    "start_time = time.time()\n",
    "test_numeric=pd.read_csv('test_numeric.csv', dtype=np.float)\n",
    "print('Load test_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 1.8 minutes\n",
    "\n",
    "test_numeric['Id']=test_numeric['Id'].astype(np.int32)\n",
    "#test_numeric['Response']=test_numeric['Response'].astype(np.int32)\n",
    "print('Process test_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "test_numeric.to_hdf('test_numeric.h5','table', complevel=1)\n",
    "print('Resave test_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dates!\n",
    "start_time = time.time()\n",
    "train_date=pd.read_csv('train_date.csv', dtype=np.float)\n",
    "print('Load train_date time: {} minutes'.format(round((time.time() - start_time)/60, 2))) \n",
    "\n",
    "train_date['Id']=train_numeric['Id'].astype(np.int32)\n",
    "print('Process train_date time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "train_date.to_hdf('train_date.h5','table', complevel=1)\n",
    "print('Resave train_date time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dates!\n",
    "start_time = time.time()\n",
    "test_date=pd.read_csv('test_date.csv', dtype=np.float)\n",
    "print('Load test_date time: {} minutes'.format(round((time.time() - start_time)/60, 2))) \n",
    "\n",
    "test_date['Id']=test_numeric['Id'].astype(np.int32)\n",
    "print('Process test_date time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "test_date.to_hdf('test_date.h5','table', complevel=1)\n",
    "print('Resave test_date time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load data and do some basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_categorical=pd.read_hdf('train_categorical_int.h5', 'table')\n",
    "train_numeric    =pd.read_hdf('train_numeric.h5', 'table')\n",
    "train_date       =pd.read_hdf('train_date.h5', 'table')\n",
    "print('Load time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load time: 0.51 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_categorical=pd.read_hdf('test_categorical_int.h5', 'table')\n",
    "test_numeric    =pd.read_hdf('test_numeric.h5', 'table')\n",
    "test_date       =pd.read_hdf('test_date.h5', 'table')\n",
    "print('Load time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "#train=pd.merge(train_categorical, train_numeric, on='Id')\n",
    "cat_names=train_categorical.columns.values[1:]\n",
    "num_names=train_numeric.columns.values[1:-1]\n",
    "dat_names=train_date.columns.values[1:]\n",
    "#print('Merge categorical and numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 2.85 minutes\n",
    "#del(train_categorical)\n",
    "#del(train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_names=test_categorical.columns.values[1:]\n",
    "num_names=test_numeric.columns.values[1:]\n",
    "dat_names=test_date.columns.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mutual_entropy(a, l):\n",
    "    n = len(a)\n",
    "    a_ent = -np.sum([x*np.log(x) for x in a.value_counts()/n])\n",
    "    l_ent = -np.sum([x*np.log(x) for x in l.value_counts()/n])\n",
    "    al_ent = -np.sum([x*np.log(x) for x in a[l==0].value_counts()/n]) - \\\n",
    "             np.sum([x*np.log(x) for x in a[l==1].value_counts()/n])\n",
    "    return a_ent + l_ent - al_ent\n",
    "def entropy(a):\n",
    "    n=len(a)\n",
    "    a_ent = -np.sum([x*np.log(x) for x in a.value_counts()/n])\n",
    "    return a_ent\n",
    "\n",
    "leaveoneout=dict()\n",
    "onehot_categorical=[]\n",
    "leaveoneout_categorical=[]\n",
    "for cat in tqdm(cat_names):\n",
    "    ent=mutual_entropy(train_categorical[cat], train_numeric['Response'])\n",
    "    if ent > 1E-5:        \n",
    "        #print(str(cat) + ': ' + str(ent))\n",
    "        if ent > 1E-3:\n",
    "            onehot_categorical = onehot_categorical + [cat]\n",
    "        leaveoneout_categorical = leaveoneout_categorical + [cat]\n",
    "        leaveoneout[cat]={k:v-1 for (k,v) in dict(train_categorical[cat].value_counts()).items()}\n",
    "selected_cat_names=list(leaveoneout.keys())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HasResponse = np.array(train_numeric['Response'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(HasResponse) / len(train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_filter=np.full(sum(HasResponse), 0, dtype=np.int)\n",
    "all_filter=np.full(len(train), 0, dtype=np.int)\n",
    "for cat in selected_cat_names:\n",
    "    positive_filter = positive_filter + np.array(train[cat][HasResponse]!=-999)\n",
    "    all_filter = all_filter + np.array(train[cat]!=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(positive_filter == 2) / sum(all_filter == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(selected_cat_names)[X[HasResponse][positive_filter == 2][3,:] != -999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutual_entropy(train['L3_S32_F3854'], train['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    sup = tp * tn - fp * fn\n",
    "    inf = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    if inf==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sup / np.sqrt(inf)\n",
    "\n",
    "def eval_mcc(y_true, y_prob, show=False):\n",
    "    idx = np.argsort(y_prob)\n",
    "    y_true_sort = y_true[idx]\n",
    "    n = y_true.shape[0]\n",
    "    nump = 1.0 * np.sum(y_true) # number of positive\n",
    "    numn = n - nump # number of negative\n",
    "    tp = nump\n",
    "    tn = 0.0\n",
    "    fp = numn\n",
    "    fn = 0.0\n",
    "    best_mcc = 0.0\n",
    "    best_id = -1\n",
    "    mccs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        # all items with idx <= i are predicted negative while others are predicted positive\n",
    "        if y_true_sort[i] == 1:\n",
    "            tp -= 1.0\n",
    "            fn += 1.0\n",
    "        else:\n",
    "            fp -= 1.0\n",
    "            tn += 1.0\n",
    "        new_mcc = mcc(tp, tn, fp, fn)\n",
    "        mccs[i] = new_mcc\n",
    "        if new_mcc >= best_mcc:\n",
    "            best_mcc = new_mcc\n",
    "            best_id = i\n",
    "    if show:\n",
    "        best_proba = y_prob[idx[best_id]]\n",
    "        y_pred = (y_prob > best_proba).astype(int)\n",
    "        score = matthews_corrcoef(y_true, y_pred)\n",
    "        print(score, best_mcc)\n",
    "        plt.plot(mccs)\n",
    "        return best_proba, best_mcc, y_pred\n",
    "    else:\n",
    "        return best_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date=train_date[dat_names].min(axis=1)\n",
    "end_date=train_date[dat_names].max(axis=1)\n",
    "duration=end_date-start_date\n",
    "start_date.fillna(-999, inplace=True)\n",
    "end_date.fillna(-999, inplace=True)\n",
    "duration.fillna(-999, inplace=True)\n",
    "date_feature=np.array([start_date.values, end_date.values, duration]).T\n",
    "np.save('date_feat.npy', date_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date=test_date[dat_names].min(axis=1)\n",
    "end_date=test_date[dat_names].max(axis=1)\n",
    "duration=end_date-start_date\n",
    "start_date.fillna(-999, inplace=True)\n",
    "end_date.fillna(-999, inplace=True)\n",
    "duration.fillna(-999, inplace=True)\n",
    "date_feature=np.array([start_date.values, end_date.values, duration]).T\n",
    "np.save('lb_date_feat.npy', date_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('numeric_feat.npz', train_numeric[num_names].fillna(-999).values)\n",
    "np.savez_compressed('categorical_feat.npz', train_categorical[cat_names].values)\n",
    "np.savez_compressed('fulldate_feat.npz', train_date[dat_names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('lb_numeric_feat.npz', test_numeric[num_names].fillna(-999).values)\n",
    "np.savez_compressed('lb_categorical_feat.npz', test_categorical[cat_names].values)\n",
    "np.savez_compressed('lb_fulldate_feat.npz', test_date[dat_names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_feat=np.load('numeric_feat.npz')['arr_0']\n",
    "categorical_feat=np.load('categorical_feat.npz')['arr_0']\n",
    "fulldate_feat=np.load('fulldate_feat.npz')['arr_0']\n",
    "date_feat=np.load('date_feat.npy')\n",
    "label=np.load('label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_feat=np.load('lb_numeric_feat.npz')['arr_0']\n",
    "categorical_feat=np.load('lb_categorical_feat.npz')['arr_0']\n",
    "fulldate_feat=np.load('lb_fulldate_feat.npz')['arr_0']\n",
    "date_feat=np.load('lb_date_feat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a=[]\n",
    "for i in tqdm(range(numeric_feat.shape[1])):\n",
    "    d0=numeric_feat[label==0,i]\n",
    "    d0=d0[d0!=-999]\n",
    "    d1=numeric_feat[label==1,i]\n",
    "    d1=d1[d1!=-999]\n",
    "    Tstat, Pval1=ttest_ind(d0, d1)\n",
    "    Tstat, Pval2=ttest_ind(d0**2, d1**2)\n",
    "    if Pval1 < 0.01 or Pval2 < 0.01:\n",
    "        a.append(True)\n",
    "    else:\n",
    "        a.append(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.array(a)\n",
    "np.save('feature_filter_ttest.npy',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=numeric_feat # np.concatenate((numeric_feat, date_feat), axis=1)\n",
    "y=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "...     X, y, stratify=y, test_size=0.3, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=ensemble.RandomForestClassifier(n_estimators=100, random_state=777, verbose=1, n_jobs=4, oob_score=True, class_weight={1:10, 0:1}) # ~10 minutes\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(X_test)[:,1]\n",
    "eval_mcc(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find multimodal distributions\n",
    "count=0\n",
    "for k in tqdm(range(len(num_names))):\n",
    "    samples=numeric_feat[label==1,k]\n",
    "    samples=samples[samples!=-999].reshape(-1,1)\n",
    "    ms=cluster.MeanShift(bandwidth=0.1, min_bin_freq=20)\n",
    "    ms.fit(samples)\n",
    "    if ms.cluster_centers_.shape[0] > 1:\n",
    "        count = count + 1\n",
    "        print(str(count) + '/' + str(k+1))\n",
    "        #print(num_names[k], end=': ')\n",
    "        #print(ms.cluster_centers_.shape[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
