{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data proprocess for fast IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load preprocessed train_categorical.csv into np.float (NaN not supported on np.int)\n",
    "start_time = time.time()\n",
    "train_categorical=pd.read_csv('train_categorical_int.csv', dtype=np.float)\n",
    "print('Load train_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 4.6 minutes\n",
    "\n",
    "# Process train_categorical into np.int32 to save memory\n",
    "# \n",
    "train_categorical.fillna(-999, inplace=True)\n",
    "train_categorical=train_categorical.astype(np.int32)\n",
    "print('Process train_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 6.25 minutes\n",
    "\n",
    "# Resave\n",
    "train_categorical.to_hdf('train_categorical_int.h5','table')\n",
    "print('Resave train_categorical time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 8.19 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load numerical features and Response!\n",
    "start_time = time.time()\n",
    "train_numeric=pd.read_csv('train_numeric.csv', dtype=np.float)\n",
    "print('Load train_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 1.8 minutes\n",
    "\n",
    "train_numeric['Id']=train_numeric['Id'].astype(np.int32)\n",
    "train_numeric['Response']=train_numeric['Response'].astype(np.int32)\n",
    "print('Process train_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "train_numeric.to_hdf('train_numeric.h5','table', complevel=1)\n",
    "print('Resave train_numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train_date time: 2.49 minutes\n",
      "Process train_date time: 2.84 minutes\n",
      "Resave train_date time: 4.36 minutes\n"
     ]
    }
   ],
   "source": [
    "# Load dates!\n",
    "start_time = time.time()\n",
    "train_date=pd.read_csv('train_date.csv', dtype=np.float)\n",
    "print('Load train_date time: {} minutes'.format(round((time.time() - start_time)/60, 2))) \n",
    "\n",
    "train_date['Id']=train_numeric['Id'].astype(np.int32)\n",
    "print('Process train_date time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "train_date.to_hdf('train_date.h5','table', complevel=1)\n",
    "print('Resave train_date time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load data and do some basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_categorical=pd.read_hdf('train_categorical_int.h5', 'table')\n",
    "train_numeric    =pd.read_hdf('train_numeric.h5', 'table')\n",
    "train_date       =pd.read_hdf('train_date.h5', 'table')\n",
    "print('Load time: {} minutes'.format(round((time.time() - start_time)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "#train=pd.merge(train_categorical, train_numeric, on='Id')\n",
    "cat_names=train_categorical.columns.values[1:]\n",
    "num_names=train_numeric.columns.values[1:-1]\n",
    "dat_names=train_date.columns.values[1:-1]\n",
    "#print('Merge categorical and numeric time: {} minutes'.format(round((time.time() - start_time)/60, 2))) # at 2.85 minutes\n",
    "#del(train_categorical)\n",
    "#del(train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2140/2140 [02:45<00:00, 12.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def mutual_entropy(a, l):\n",
    "    n = len(a)\n",
    "    a_ent = -np.sum([x*np.log(x) for x in a.value_counts()/n])\n",
    "    l_ent = -np.sum([x*np.log(x) for x in l.value_counts()/n])\n",
    "    al_ent = -np.sum([x*np.log(x) for x in a[l==0].value_counts()/n]) - \\\n",
    "             np.sum([x*np.log(x) for x in a[l==1].value_counts()/n])\n",
    "    return a_ent + l_ent - al_ent\n",
    "def entropy(a):\n",
    "    n=len(a)\n",
    "    a_ent = -np.sum([x*np.log(x) for x in a.value_counts()/n])\n",
    "    return a_ent\n",
    "\n",
    "leaveoneout=dict()\n",
    "onehot_categorical=[]\n",
    "leaveoneout_categorical=[]\n",
    "for cat in tqdm(cat_names):\n",
    "    ent=mutual_entropy(train_categorical[cat], train_numeric['Response'])\n",
    "    if ent > 1E-5:        \n",
    "        #print(str(cat) + ': ' + str(ent))\n",
    "        if ent > 1E-3:\n",
    "            onehot_categorical = onehot_categorical + [cat]\n",
    "        leaveoneout_categorical = leaveoneout_categorical + [cat]\n",
    "        leaveoneout[cat]={k:v-1 for (k,v) in dict(train_categorical[cat].value_counts()).items()}\n",
    "selected_cat_names=list(leaveoneout.keys())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HasResponse = np.array(train_numeric['Response'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0058112079692704604"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(HasResponse) / len(train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_filter=np.full(sum(HasResponse), 0, dtype=np.int)\n",
    "all_filter=np.full(len(train), 0, dtype=np.int)\n",
    "for cat in selected_cat_names:\n",
    "    positive_filter = positive_filter + np.array(train[cat][HasResponse]!=-999)\n",
    "    all_filter = all_filter + np.array(train[cat]!=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042234251383023023"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(positive_filter == 2) / sum(all_filter == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L3_S32_F3854', 'L3_S32_F3851'], \n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(selected_cat_names)[X[HasResponse][positive_filter == 2][3,:] != -999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020189238473217186"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_entropy(train['L3_S32_F3854'], train['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    sup = tp * tn - fp * fn\n",
    "    inf = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    if inf==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sup / np.sqrt(inf)\n",
    "\n",
    "def eval_mcc(y_true, y_prob, show=False):\n",
    "    idx = np.argsort(y_prob)\n",
    "    y_true_sort = y_true[idx]\n",
    "    n = y_true.shape[0]\n",
    "    nump = 1.0 * np.sum(y_true) # number of positive\n",
    "    numn = n - nump # number of negative\n",
    "    tp = nump\n",
    "    tn = 0.0\n",
    "    fp = numn\n",
    "    fn = 0.0\n",
    "    best_mcc = 0.0\n",
    "    best_id = -1\n",
    "    mccs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        # all items with idx <= i are predicted negative while others are predicted positive\n",
    "        if y_true_sort[i] == 1:\n",
    "            tp -= 1.0\n",
    "            fn += 1.0\n",
    "        else:\n",
    "            fp -= 1.0\n",
    "            tn += 1.0\n",
    "        new_mcc = mcc(tp, tn, fp, fn)\n",
    "        mccs[i] = new_mcc\n",
    "        if new_mcc >= best_mcc:\n",
    "            best_mcc = new_mcc\n",
    "            best_id = i\n",
    "    if show:\n",
    "        best_proba = y_prob[idx[best_id]]\n",
    "        y_pred = (y_prob > best_proba).astype(int)\n",
    "        score = matthews_corrcoef(y_true, y_pred)\n",
    "        print(score, best_mcc)\n",
    "        plt.plot(mccs)\n",
    "        return best_proba, best_mcc, y_pred\n",
    "    else:\n",
    "        return best_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jxy198/code/scikit-learn/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=train_date.fillna(-999).values\n",
    "y=train_numeric['Response'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "...     X, y, stratify=y, test_size=0.5, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=ensemble.RandomForestClassifier(n_estimators=100, random_state=777, verbose=1, n_jobs=4, oob_score=True, class_weight={1:10, 0:1}) # ~10 minutes\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(X_test)[:,1]\n",
    "eval_mcc(y_test, y_pred, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_num_names=num_names[clf.feature_importances_ > 0.0001]\n",
    "len(selected_num_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
